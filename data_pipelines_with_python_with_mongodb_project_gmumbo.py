# -*- coding: utf-8 -*-
"""Data Pipelines with Python with MongoDB Project_GMumbo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M_VnFFxl1UoICt6bnopGOORS3qWxmiFh

##Telecommunications Fraud Detection Using MongoDB and Python

Project Deliverable

● A GitHub repository with a python file (.py) with your solution.

Problem Statement

Telecommunications companies need to detect fraudulent activities such as unauthorized use of
premium services or fake billing. Building a data pipeline with MongoDB and Python could help
identify suspicious activity by extracting data from billing systems, call logs, and other sources,
transforming the data to identify patterns or anomalies, and storing it in MongoDB for further
analysis.

Background Information

Telecommunications companies generate a vast amount of data daily, which can be used to
detect fraud. Fraudulent activity can lead to substantial financial losses and damage the
company's reputation. With the help of data pipelines, companies can detect fraud before it
escalates.

Guidelines

We will build a data pipeline with three main functions: extraction, transformation, and loading.
The pipeline will extract data from CSV files, transform it to identify suspicious activities, and
load it into MongoDB. We will use Python as the programming language, and Pymongo as the
driver to interact with MongoDB.

● Sample Datasets for Extraction: We will use sample call log data in CSV format as the
dataset for extraction. The dataset will include fields such as call duration, call type,
phone number, and time stamp.

● Extraction Function: The extraction function will read data from CSV files and insert it
into MongoDB. To optimize the data pipeline, we can use connection pooling to maintain
open connections to the database. To secure the pipeline, we can use SSL encryption to
encrypt the data transmitted between the client and server. We can also use logging to
monitor the extraction process for errors and potential security breaches. Here are some
guidelines for the extraction function:
○ Use the pandas library to read the input CSV files.
○ Clean and preprocess the data by removing duplicates, handling missing values,
and converting data types.

○ Use connection pooling to optimize performance.

○ Use SSL encryption to secure the pipeline.

○ Log errors and activities using the Python logging module.

● Transformation Function: The transformation function will identify suspicious activities
based on the data extracted from CSV files. We can use techniques such as aggregation
and grouping to identify patterns and anomalies in the data. To optimize the data
pipeline, we can use indexes to speed up the data retrieval process. To secure the
pipeline, we can use data masking to protect sensitive data. Here are some guidelines
for the transformation function:
○ Clean the data and handle missing values.
○ Group and aggregate the data by customer, location, time, and other relevant
parameters.
○ Identify patterns in the data to detect suspicious activity, such as unauthorized
use of premium services, fake billing, or international calls.
○ Use data compression techniques to optimize performance and reduce storage
requirements.
○ Use the Python logging module to log errors and activities.

● Loading Function: The loading function will insert the transformed data into MongoDB.
We can use batch inserts to improve performance and reduce network traffic. To
optimize the data pipeline, we can use sharding to distribute the data across multiple
shards and balance the load. To secure the pipeline, we can use authentication and
authorization to restrict access to the data. Here are some guidelines for the loading

function:
○ Use the pymongo library to connect to the MongoDB instance.
○ Create a new MongoDB collection for each data source.
○ Create indexes on the collection to optimize queries and performance.
○ Use bulk inserts to optimize performance.
○ Use the write concern option to ensure that data is written to disk.
○ Use the Python logging module to log errors and activities.
You can use the guiding file (https://bit.ly/3lV9fW3) as a starting point for your data pipeline.

Sample Datasets for Data Extraction

Here are some sample datasets (https://bit.ly/3YRn7z4) you can use for extraction:
● Call logs
● Billing systems

##Import Libraries
"""

!pip install psycopg2-binary

import pandas as pd
import pymongo
import logging
from pymongo import UpdateOne, DeleteOne
from pymongo.errors import BulkWriteError
from pprint import pprint

"""##Data Extraction"""

# Extraction function
def extract_call_logs():
    # Load call log data from CSV file
    call_logs = pd.read_csv('call_logs.csv')

    call_logs['duration_minutes'] = call_logs['call_duration'] / 60

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Call logs extraction completed.")

    return call_logs

def extract_billing_data()

    # Load billing data from CSV file
    billing_data = pd.read_csv('billing_data.csv')

    # Merge the two datasets based on common columns
    merged_data = pd.merge(call_logs, billing_data, on=['phone_number', 'call_date'])

    # Convert call duration to minutes for easier analysis
    merged_data['duration_minutes'] = merged_data['call_duration'] / 60

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Billing Data extraction completed.")

    return merged_data

"""##Transformation"""

# Transformation function
def transform_call_logs(call_logs):
    # Data cleaning and handling missing values
    # ...
    transformed_data = call_logs.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Group and aggregate the data
    # ...
    logger = logging.getLogger(__name__)
    logger.info("Call logs transformation completed.")

    # Identify patterns in the data
    # ...
    transformed_data = transformed_data.to_dict('records')
    
    return transformed_data

    # Use data compression techniques to optimize performance
    # ...
def transform_billing_systems(billing_systems):
    # Data cleaning and handling missing values
    transformed_data = billing_systems.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Use Python logging module to log errors and activities
    
    logger = logging.getLogger(__name__)
    logger.info("Billing systems transformation completed.")
    
    transformed_data = transformed_data.to_dict('records')

    return transformed_data

"""##Loading"""

# Loading function

def load_data(merged_data):
    # Connect to MongoDB
    client = mongodb+srv://gmumbo:<password>@cluster0.wbvkddi.mongodb.net/?retryWrites=true&w=majority
    db = client["gmumbo"]
    collection = db["gmumbo"]

    # Create indexes on the collection
    # ...
    collection.create_index([('call_duration',pymongo.DESCENDING)],
                            storageEngine={
                                'wiredTiger': {
                                    'configString': 'block_compressor=snappy'
                                }
                            }
                           )

    # Use bulk inserts to optimize performance
    collection.insert_many(merged_data)

    # Use the write concern option to ensure that data is written to disk
    collection.acknowledge_writes(w=1, j=True)

    requests = [
        UpdateOne({"call_id":1},{'$set':{'call_type':'Incoming'}}),
        DeleteOne({'call_id':2})
    ]
    try:
        collection.bulk_write(requests)
    except BulkWriteError as bwe:
        pprint(bwe.details)

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Data loading completed.")

# Main Function
if __name__ == '__main__':

    call_logs = extract_call_logs()
    billing_data = extract_billing_data()

    transformed_call_logs = transform_call_logs(call_logs)
    transformed_data = transform_data(billing_data)

    # Merge the two dataframes
    merged_data = transformed_call_logs + transformed_data

    load_data(merged_data)

